{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Bidirectional, merge, dot, Dense, Embedding, Input, Lambda, LSTM, RepeatVector, TimeDistributed, Layer, Activation, Dropout\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers.advanced_activations import ELU\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.optimizers import Adam\n",
    "from keras import backend as K\n",
    "from keras.models import Model\n",
    "from scipy import spatial\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import codecs\n",
    "import csv\n",
    "import os\n",
    "import scipy.sparse\n",
    "import random\n",
    "import itertools\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "TRAIN_DATA_FILE = '/data/chzho/deepqts/train_data/unifiedclick/join_oneyearsample_2B_training_all_top10'\n",
    "batch_size = 1000\n",
    "MAX_SEQUENCE_LENGTH = 7\n",
    "MAX_NB_WORDS = 100000\n",
    "max_features = 50000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8.68 s, sys: 656 ms, total: 9.34 s\n",
      "Wall time: 9.33 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "num_read_row = 1000000\n",
    "df = pd.read_csv(TRAIN_DATA_FILE, sep=\"\\t\", usecols=[0,1,3], names=['label', 'q', 'd'], header=None , error_bad_lines=False, nrows=num_read_row)\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def text_generator(TRAIN_DATA_FILE, batch_size):\n",
    "    reader = pd.read_csv(TRAIN_DATA_FILE, chunksize=batch_size, iterator=True, sep=\"\\t\", usecols=[0,1,3], names=['label', 'q', 'd'], header=None , error_bad_lines=False)\n",
    "    for df in reader:\n",
    "        yield df.iloc[:,1].tolist() + df.iloc[:,2].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 24.4 s, sys: 1.04 s, total: 25.5 s\n",
      "Wall time: 25.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "count_vect = CountVectorizer(max_features=max_features)\n",
    "x_train = count_vect.fit_transform(df.q.tolist() + df.d.tolist())\n",
    "tf_transformer = TfidfTransformer().fit(x_train)\n",
    "x_train = tf_transformer.transform(x_train)\n",
    "y_train = df.label.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "q_train = x_train[:len(df)]\n",
    "d_train = x_train[len(df):]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split 90% of data as unsupervise data and 10% of data as supervised data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 12.6 s, sys: 55.2 s, total: 1min 7s\n",
      "Wall time: 1min 7s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "sample_num = 100000\n",
    "sup_x_train = np.concatenate((q_train[:sample_num].todense(), d_train[:sample_num].todense()), axis=1)\n",
    "sup_y_train = y_train[:sample_num]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "# uns_x_train = np.concatenate((q_train[sample_num:].todense(), d_train[sample_num:].todense()))\n",
    "# uns_y_train = uns_x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from utils import *\n",
    "from VDSH import *\n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "\n",
    "latent_dim = 32\n",
    "sess = get_session(\"0\") # choose the GPU and how much memory in percentage that we need\n",
    "model = VDSH(sess, latent_dim, max_features)\n",
    "\n",
    "# create an optimizer\n",
    "learning_rate=0.001\n",
    "decay_rate = 0.96\n",
    "#decay_step = 10000\n",
    "step = tf.Variable(0, trainable=False)  \n",
    "lr = tf.train.exponential_decay(learning_rate, \n",
    "                                step, \n",
    "                                10000, \n",
    "                                decay_rate, \n",
    "                                staircase=True, name=\"lr\")\n",
    "\n",
    "my_optimizer = tf.train.AdamOptimizer(learning_rate=lr) \\\n",
    "                     .minimize(model.cost, global_step=step)\n",
    "    \n",
    "init = tf.global_variables_initializer()\n",
    "model.sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 112 ms, sys: 88 ms, total: 200 ms\n",
      "Wall time: 196 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "uns_q_train = q_train[sample_num:]\n",
    "uns_d_train = d_train[sample_num:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:4/25 291050/900000: Loss:19.924 AvgLoss:48.38781"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "total_epoch = 25\n",
    "kl_weight = 0.\n",
    "kl_inc = 1 / 5000. # set the annealing rate for KL loss\n",
    "\n",
    "for epoch in range(total_epoch):\n",
    "    epoch_loss = []\n",
    "    for i in range(900000):\n",
    "        # get doc\n",
    "        doc =  uns_q_train[i].todense()\n",
    "        word_indice = np.where(doc > 0)[1]\n",
    "        \n",
    "        # indices\n",
    "        opt, loss = model.sess.run((my_optimizer, model.cost), \n",
    "                                    feed_dict={model.input_bow: doc.reshape((-1, max_features)),\n",
    "                                               model.input_bow_idx: word_indice,\n",
    "                                               model.kl_weight: kl_weight,\n",
    "                                               model.keep_prob: 0.9})\n",
    "        \n",
    "        kl_weight = min(kl_weight + kl_inc, 1.0)\n",
    "        epoch_loss.append(loss)\n",
    "        \n",
    "        \n",
    "        # get doc\n",
    "        doc =  uns_d_train[i].todense()\n",
    "        word_indice = np.where(doc > 0)[1]\n",
    "        \n",
    "        # indices\n",
    "        opt, loss = model.sess.run((my_optimizer, model.cost), \n",
    "                                    feed_dict={model.input_bow: doc.reshape((-1, max_features)),\n",
    "                                               model.input_bow_idx: word_indice,\n",
    "                                               model.kl_weight: kl_weight,\n",
    "                                               model.keep_prob: 0.9})\n",
    "        \n",
    "        kl_weight = min(kl_weight + kl_inc, 1.0)\n",
    "        epoch_loss.append(loss)\n",
    "        \n",
    "        if i % 50 == 0:\n",
    "            print(\"\\rEpoch:{}/{} {}/{}: Loss:{:.3f} AvgLoss:{:.3f}\"\n",
    "                  .format(epoch+1, total_epoch, i, 900000, loss, np.mean(epoch_loss)), end='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### encoder queries and documents for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "enc_q = model.transform(q_train[:sample_num].todense())\n",
    "enc_d = model.transform(d_train[:sample_num].todense())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "enc_q = np.array(enc_q)\n",
    "enc_d = np.array(enc_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "import numpy as np\n",
    "# fix random seed for reproducibility\n",
    "np.random.seed(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create model\n",
    "input_dim = q_train.shape[1] + d_train.shape[1]\n",
    "model = Sequential()\n",
    "model.add(Dense(512, input_dim=input_dim, activation='relu'))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Compile model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 80000 samples, validate on 20000 samples\n",
      "Epoch 1/1\n",
      " - 166s - loss: 0.6172 - acc: 0.6273 - val_loss: 0.5975 - val_acc: 0.6575\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f24187b4ef0>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(small_x_train, small_y_train, batch_size=batch_size, validation_split=0.2, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "53080"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.count_nonzero(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100000"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import random\n",
    "p = 0.01  # 1% of the lines\n",
    "random_df = pd.read_csv(TRAIN_DATA_FILE, sep=\"\\t\", usecols=[0,1,3], names=['label', 'q', 'd'], header=None , error_bad_lines=False, nrows=50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda3/lib/python3.6/site-packages/ipykernel/__main__.py:17: UserWarning: The `merge` function is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "/home/t-jamano/.local/lib/python3.6/site-packages/keras/legacy/layers.py:464: UserWarning: The `Merge` layer is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "  name=name)\n",
      "/usr/local/anaconda3/lib/python3.6/site-packages/ipykernel/__main__.py:29: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=[<tf.Tenso..., outputs=Tensor(\"de...)`\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras import backend as K\n",
    "from keras.models import Sequential,  Model\n",
    "from keras.layers.core import Dense, Lambda, Activation\n",
    "from keras.layers import Embedding, Input, Dense, merge, Reshape, Merge, Flatten, Dropout, GlobalAveragePooling1D\n",
    "from keras.constraints import maxnorm\n",
    "from keras.optimizers import Adagrad, Adam, SGD, RMSprop\n",
    "from time import time\n",
    "import sys\n",
    "\n",
    "# que_input = Input(shape=(max_features,))\n",
    "# doc_input = Input(shape=(max_features,))\n",
    "\n",
    "que_input = Input(shape=(latent_dim,))\n",
    "doc_input = Input(shape=(latent_dim,))\n",
    "\n",
    "concat = merge([que_input, doc_input], mode=\"concat\")\n",
    "\n",
    "d1 = Dense(512, input_dim=input_dim, activation='relu')\n",
    "d2 = Dense(256, activation='relu')\n",
    "d3 = Dense(128, activation='relu')\n",
    "d4 = Dense(64, activation='relu')\n",
    "d5 = Dense(32, activation='relu')\n",
    "d6 = Dense(1, activation='sigmoid')\n",
    "\n",
    "out = d6(d5(d4(d3(d2(d1(concat))))))\n",
    "\n",
    "\n",
    "model2 = Model(input=[que_input, doc_input], output=out)\n",
    "\n",
    "model2.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF-IDF features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 80000 samples, validate on 20000 samples\n",
      "Epoch 1/1\n",
      " - 179s - loss: 0.6191 - acc: 0.6275 - val_loss: 0.6011 - val_acc: 0.6517\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f6f7b610ba8>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.fit([q_train[:sample_num].todense(), d_train[:sample_num].todense()], sup_y_train, batch_size=batch_size, validation_split=0.2, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VAE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 80000 samples, validate on 20000 samples\n",
      "Epoch 1/10\n",
      " - 1s - loss: 0.6912 - acc: 0.5312 - val_loss: 0.6915 - val_acc: 0.5293\n",
      "Epoch 2/10\n",
      " - 1s - loss: 0.6913 - acc: 0.5312 - val_loss: 0.6915 - val_acc: 0.5293\n",
      "Epoch 3/10\n",
      " - 1s - loss: 0.6912 - acc: 0.5312 - val_loss: 0.6914 - val_acc: 0.5293\n",
      "Epoch 4/10\n",
      " - 1s - loss: 0.6912 - acc: 0.5312 - val_loss: 0.6915 - val_acc: 0.5293\n",
      "Epoch 5/10\n",
      " - 1s - loss: 0.6912 - acc: 0.5312 - val_loss: 0.6914 - val_acc: 0.5293\n",
      "Epoch 6/10\n",
      " - 1s - loss: 0.6912 - acc: 0.5312 - val_loss: 0.6915 - val_acc: 0.5293\n",
      "Epoch 7/10\n",
      " - 1s - loss: 0.6912 - acc: 0.5312 - val_loss: 0.6914 - val_acc: 0.5293\n",
      "Epoch 8/10\n",
      " - 1s - loss: 0.6912 - acc: 0.5312 - val_loss: 0.6915 - val_acc: 0.5293\n",
      "Epoch 9/10\n",
      " - 1s - loss: 0.6912 - acc: 0.5312 - val_loss: 0.6916 - val_acc: 0.5293\n",
      "Epoch 10/10\n",
      " - 0s - loss: 0.6912 - acc: 0.5312 - val_loss: 0.6914 - val_acc: 0.5293\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f39d8033cf8>"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.fit([enc_q, enc_d], sup_y_train, batch_size=batch_size, validation_split=0.2, verbose=2, epochs=10)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

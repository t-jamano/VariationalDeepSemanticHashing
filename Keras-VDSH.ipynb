{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.layers import Bidirectional, merge, dot, Dense, Embedding, Input, Lambda, LSTM, RepeatVector, TimeDistributed, Layer, Activation, Dropout\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.losses import mse, binary_crossentropy\n",
    "from keras.layers.advanced_activations import ELU\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.optimizers import Adam\n",
    "from keras import backend as K\n",
    "from keras.models import Model\n",
    "from scipy import spatial\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import codecs\n",
    "import csv\n",
    "import os\n",
    "import scipy.sparse\n",
    "import random\n",
    "import itertools\n",
    "import math\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TRAIN_DATA_FILE = '/data/chzho/deepqts/train_data/unifiedclick/join_oneyearsample_2B_training_all_top10'\n",
    "batch_size = 1000\n",
    "MAX_SEQUENCE_LENGTH = 7\n",
    "MAX_NB_WORDS = 100000\n",
    "max_features = 50000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7.3 s, sys: 29.1 s, total: 36.4 s\n",
      "Wall time: 45.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "num_read_row = 1000000\n",
    "df = pd.read_csv(TRAIN_DATA_FILE, sep=\"\\t\", usecols=[0,1,3], names=['label', 'q', 'd'], header=None , error_bad_lines=False, nrows=num_read_row)\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py:1059: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  if hasattr(X, 'dtype') and np.issubdtype(X.dtype, np.float):\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "count_vect = CountVectorizer(max_features=max_features)\n",
    "x_train = count_vect.fit_transform(df.q.tolist() + df.d.tolist())\n",
    "tf_transformer = TfidfTransformer().fit(x_train)\n",
    "x_train = tf_transformer.transform(x_train)\n",
    "y_train = df.label.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "q_train = x_train[:len(df)]\n",
    "d_train = x_train[len(df):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "# sample_num = 100000\n",
    "# sup_x_train = np.concatenate((q_train[:sample_num].todense(), d_train[:sample_num].todense()), axis=1)\n",
    "# sup_y_train = y_train[:sample_num]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class VAE():\n",
    "    def __init__(self, latent_dim, hidden_dim, feature_num):\n",
    "        \n",
    "        self.latent_dim = latent_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.feature_num = feature_num\n",
    "    \n",
    "    def transform(self, docs):\n",
    "        return self.encoder.predict(docs)\n",
    "    \n",
    "    def build(self):\n",
    "        \n",
    "        def sampling(args):\n",
    "            \n",
    "            \"\"\"Reparameterization trick by sampling fr an isotropic unit Gaussian.\n",
    "            # Arguments:\n",
    "                args (tensor): mean and log of variance of Q(z|X)\n",
    "            # Returns:\n",
    "                z (tensor): sampled latent vector\n",
    "            \"\"\"\n",
    "            z_mean, z_log_var = args\n",
    "            batch = K.shape(z_mean)[0]\n",
    "            dim = K.int_shape(z_mean)[1]\n",
    "            # by default, random_normal has mean=0 and std=1.0\n",
    "            epsilon = K.random_normal(shape=(batch, dim))\n",
    "            return z_mean + K.exp(0.5 * z_log_var) * epsilon\n",
    "        \n",
    "        # VAE model = encoder + decoder\n",
    "        # build encoder model\n",
    "        inputs = Input(shape=(self.feature_num, ), name='encoder_input')\n",
    "        x = Dense(self.hidden_dim, activation='relu')(inputs)\n",
    "        z_mean = Dense(self.latent_dim, name='z_mean')(x)\n",
    "        z_log_var = Dense(self.latent_dim, name='z_log_var')(x)\n",
    "\n",
    "        # use reparameterization trick to push the sampling out as input\n",
    "        # note that \"output_shape\" isn't necessary with the TensorFlow backend\n",
    "        z = Lambda(sampling, output_shape=(self.latent_dim,), name='z')([z_mean, z_log_var])\n",
    "\n",
    "        # instantiate encoder model\n",
    "        self.encoder = Model(inputs, z, name='encoder')\n",
    "\n",
    "        # build decoder model\n",
    "        latent_inputs = Input(shape=(self.latent_dim, ), name='z_sampling')\n",
    "        x = Dense(self.hidden_dim, activation='relu')(latent_inputs)\n",
    "        outputs = Dense(self.feature_num, activation='sigmoid')(x)\n",
    "\n",
    "        # instantiate decoder model\n",
    "        self.decoder = Model(latent_inputs, outputs, name='decoder')\n",
    "\n",
    "\n",
    "        # instantiate VAE model\n",
    "        outputs = self.decoder(self.encoder(inputs))\n",
    "        self.model = Model(inputs, outputs, name='vae_mlp')\n",
    "        \n",
    "        reconstruction_loss = binary_crossentropy(inputs,\n",
    "                                                  outputs)\n",
    "        reconstruction_loss *= self.feature_num\n",
    "        kl_loss = 1 + z_log_var - K.square(z_mean) - K.exp(z_log_var)\n",
    "        kl_loss = K.sum(kl_loss, axis=-1)\n",
    "        kl_loss *= -0.5\n",
    "        vae_loss = K.mean(reconstruction_loss + kl_loss)\n",
    "        self.model.add_loss(vae_loss)\n",
    "        self.model.compile(optimizer='adam')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vae = VAE(32,500, 50000)\n",
    "vae.build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "\r",
      "64/64 [==============================] - 0s 1ms/step - loss: 4322577.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f86c7ba14a8>"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_ = np.random.randint(2, size=(batch_size, vae.feature_num))\n",
    "vae.model.fit(x_, batch_size=batch_size, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 84 ms, sys: 436 ms, total: 520 ms\n",
      "Wall time: 518 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "sample_num = 100000\n",
    "uns_q_train = q_train[sample_num:]\n",
    "uns_d_train = d_train[sample_num:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1563/1563 [01:44<00:00, 14.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 18s, sys: 22.7 s, total: 1min 41s\n",
      "Wall time: 1min 44s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "x = q_train[:100000]\n",
    "batch_size = 64\n",
    "\n",
    "for i in tqdm(range(math.ceil(x.shape[0]/batch_size))):\n",
    "    batch_q = uns_q_train[i*batch_size:(i+1)*batch_size].todense()\n",
    "    batch_d = uns_d_train[i*batch_size:(i+1)*batch_size].todense()\n",
    "    \n",
    "    vae.model.train_on_batch(batch_d, [])\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "sample_num = 100000\n",
    "sup_x_train = np.concatenate((q_train[:sample_num].todense(), d_train[:sample_num].todense()), axis=1)\n",
    "sup_y_train = y_train[:sample_num]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras import backend as K\n",
    "from keras.models import Sequential,  Model\n",
    "from keras.layers.core import Dense, Lambda, Activation\n",
    "from keras.layers import Embedding, Input, Dense, merge, Reshape, Merge, Flatten, Dropout, GlobalAveragePooling1D\n",
    "from keras.constraints import maxnorm\n",
    "from keras.optimizers import Adagrad, Adam, SGD, RMSprop\n",
    "from time import time\n",
    "import sys\n",
    "\n",
    "# que_input = Input(shape=(max_features,))\n",
    "# doc_input = Input(shape=(max_features,))\n",
    "\n",
    "que_input = Input(shape=(latent_dim,))\n",
    "doc_input = Input(shape=(latent_dim,))\n",
    "\n",
    "concat = merge([que_input, doc_input], mode=\"concat\")\n",
    "\n",
    "d1 = Dense(512, input_dim=input_dim, activation='relu')\n",
    "d2 = Dense(256, activation='relu')\n",
    "d3 = Dense(128, activation='relu')\n",
    "d4 = Dense(64, activation='relu')\n",
    "d5 = Dense(32, activation='relu')\n",
    "d6 = Dense(1, activation='sigmoid')\n",
    "\n",
    "out = d6(d5(d4(d3(d2(d1(concat))))))\n",
    "\n",
    "\n",
    "model2 = Model(input=[que_input, doc_input], output=out)\n",
    "\n",
    "model2.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in tqdm(range(math.ceil(x.shape[0]/batch_size))):\n",
    "    batch_q = q_train[i*batch_size:(i+1)*batch_size].todense()\n",
    "    batch_d = d_train[i*batch_size:(i+1)*batch_size].todense()\n",
    "    \n",
    "    model.fit(small_x_train, small_y_train, batch_size=batch_size, verbose=0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Bidirectional, merge, Flatten, dot, Dense, Embedding, Input, Lambda, LSTM, RepeatVector, TimeDistributed, Layer, Activation, Dropout\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.losses import mse, binary_crossentropy\n",
    "from keras.layers.advanced_activations import ELU\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.optimizers import Adam\n",
    "from keras import backend as K\n",
    "from keras.models import Model\n",
    "from scipy import spatial\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import codecs\n",
    "import csv\n",
    "import os\n",
    "import scipy.sparse\n",
    "import random\n",
    "import itertools\n",
    "import math\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TRAIN_DATA_FILE = '/data/chzho/deepqts/train_data/unifiedclick/join_oneyearsample_2B_training_all_top10'\n",
    "batch_size = 1000\n",
    "MAX_SEQUENCE_LENGTH = 7\n",
    "MAX_NB_WORDS = 1000000\n",
    "max_features = 50000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 26s, sys: 6.18 s, total: 1min 32s\n",
      "Wall time: 1min 32s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "num_read_row = 10000000\n",
    "df = pd.read_csv(TRAIN_DATA_FILE, sep=\"\\t\", usecols=[0,1,3], names=['label', 'q', 'd'], header=None , error_bad_lines=False, nrows=num_read_row)\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 25.3 s, sys: 760 ms, total: 26.1 s\n",
      "Wall time: 26.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "count_vect = CountVectorizer(max_features=max_features)\n",
    "x_train = count_vect.fit_transform(df.q.tolist() + df.d.tolist())\n",
    "tf_transformer = TfidfTransformer().fit(x_train)\n",
    "x_train = tf_transformer.transform(x_train)\n",
    "y_train = df.label.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<2000000x50000 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 7974826 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "q_train = x_train[:len(df)]\n",
    "d_train = x_train[len(df):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "# sample_num = 100000\n",
    "# sup_x_train = np.concatenate((q_train[:sample_num].todense(), d_train[:sample_num].todense()), axis=1)\n",
    "# sup_y_train = y_train[:sample_num]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class VAE():\n",
    "    def __init__(self, latent_dim, hidden_dim, feature_num):\n",
    "        \n",
    "        self.latent_dim = latent_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.feature_num = feature_num\n",
    "    \n",
    "    def transform(self, docs):\n",
    "        return self.encoder.predict(docs)\n",
    "    \n",
    "    def build(self):\n",
    "        \n",
    "        def sampling(args):\n",
    "            \n",
    "            \"\"\"Reparameterization trick by sampling fr an isotropic unit Gaussian.\n",
    "            # Arguments:\n",
    "                args (tensor): mean and log of variance of Q(z|X)\n",
    "            # Returns:\n",
    "                z (tensor): sampled latent vector\n",
    "            \"\"\"\n",
    "            z_mean, z_log_var = args\n",
    "            batch = K.shape(z_mean)[0]\n",
    "            dim = K.int_shape(z_mean)[1]\n",
    "            # by default, random_normal has mean=0 and std=1.0\n",
    "            epsilon = K.random_normal(shape=(batch, dim))\n",
    "            return z_mean + K.exp(0.5 * z_log_var) * epsilon\n",
    "        \n",
    "        # VAE model = encoder + decoder\n",
    "        # build encoder model\n",
    "        inputs = Input(shape=(self.feature_num, ), name='encoder_input')\n",
    "        x = Flatten()(embed(inputs))\n",
    "        x = Dense(self.hidden_dim, activation='relu')(x)\n",
    "        z_mean = Dense(self.latent_dim, name='z_mean')(x)\n",
    "        z_log_var = Dense(self.latent_dim, name='z_log_var')(x)\n",
    "\n",
    "        # use reparameterization trick to push the sampling out as input\n",
    "        # note that \"output_shape\" isn't necessary with the TensorFlow backend\n",
    "        z = Lambda(sampling, output_shape=(self.latent_dim,), name='z')([z_mean, z_log_var])\n",
    "\n",
    "        # instantiate encoder model\n",
    "        self.encoder = Model(inputs, z, name='encoder')\n",
    "\n",
    "        # build decoder model\n",
    "        latent_inputs = Input(shape=(self.latent_dim, ), name='z_sampling')\n",
    "        x = Dense(self.hidden_dim, activation='relu')(latent_inputs)\n",
    "        outputs = Dense(self.feature_num, activation='sigmoid')(x)\n",
    "\n",
    "        # instantiate decoder model\n",
    "        self.decoder = Model(latent_inputs, outputs, name='decoder')\n",
    "\n",
    "\n",
    "        # instantiate VAE model\n",
    "        outputs = self.decoder(self.encoder(inputs))\n",
    "        self.model = Model(inputs, outputs, name='vae_mlp')\n",
    "        \n",
    "        reconstruction_loss = binary_crossentropy(inputs,\n",
    "                                                  outputs)\n",
    "        reconstruction_loss *= self.feature_num\n",
    "        kl_loss = 1 + z_log_var - K.square(z_mean) - K.exp(z_log_var)\n",
    "        kl_loss = K.sum(kl_loss, axis=-1)\n",
    "        kl_loss *= -0.5\n",
    "        vae_loss = K.mean(reconstruction_loss + kl_loss)\n",
    "        self.model.add_loss(vae_loss)\n",
    "        self.model.compile(optimizer='adam')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda3/lib/python3.6/site-packages/ipykernel/__main__.py:64: UserWarning: Output \"decoder\" missing from loss dictionary. We assume this was done on purpose, and we will not be expecting any data to be passed to \"decoder\" during training.\n"
     ]
    }
   ],
   "source": [
    "vae = VAE(200,1400, 7)\n",
    "vae.build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 124 ms, sys: 88 ms, total: 212 ms\n",
      "Wall time: 211 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "sample_num = 100000\n",
    "uns_q_train = q_train[sample_num:]\n",
    "uns_d_train = d_train[sample_num:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<900000x50000 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 2571627 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uns_q_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1563/1563 [01:35<00:00, 16.36it/s]\n",
      "100%|██████████| 1250/1250 [01:58<00:00, 10.51it/s]\n",
      "100%|██████████| 2000/2000 [00:39<00:00, 50.58it/s]\n",
      "  0%|          | 2/1563 [00:00<01:50, 14.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.508125012428\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1563/1563 [01:35<00:00, 16.29it/s]\n",
      "100%|██████████| 1250/1250 [01:57<00:00, 10.66it/s]\n",
      "100%|██████████| 2000/2000 [00:38<00:00, 52.79it/s]\n",
      "  0%|          | 2/1563 [00:00<01:42, 15.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.51920001292\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1563/1563 [01:35<00:00, 16.30it/s]\n",
      "100%|██████████| 1250/1250 [01:56<00:00, 10.69it/s]\n",
      "100%|██████████| 2000/2000 [00:39<00:00, 50.41it/s]\n",
      "  0%|          | 2/1563 [00:00<01:44, 14.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.524500013161\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1563/1563 [01:35<00:00, 16.28it/s]\n",
      "100%|██████████| 1250/1250 [01:57<00:00, 10.46it/s]\n",
      "100%|██████████| 2000/2000 [00:39<00:00, 50.72it/s]\n",
      "  0%|          | 2/1563 [00:00<01:47, 14.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.527920013409\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1563/1563 [01:35<00:00, 16.39it/s]\n",
      "100%|██████████| 1250/1250 [01:56<00:00, 10.70it/s]\n",
      "100%|██████████| 2000/2000 [00:39<00:00, 50.90it/s]\n",
      "  0%|          | 2/1563 [00:00<02:05, 12.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.532216680278\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1563/1563 [01:35<00:00, 16.43it/s]\n",
      "100%|██████████| 1250/1250 [01:55<00:00, 10.71it/s]\n",
      "100%|██████████| 2000/2000 [00:39<00:00, 50.84it/s]\n",
      "  0%|          | 2/1563 [00:00<01:58, 13.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.535585728075\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1563/1563 [01:35<00:00, 16.36it/s]\n",
      "100%|██████████| 1250/1250 [01:56<00:00, 10.34it/s]\n",
      "100%|██████████| 2000/2000 [00:38<00:00, 51.44it/s]\n",
      "  0%|          | 2/1563 [00:00<01:43, 15.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.538343763873\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1563/1563 [01:35<00:00, 16.37it/s]\n",
      "100%|██████████| 1250/1250 [01:56<00:00, 10.69it/s]\n",
      "100%|██████████| 2000/2000 [00:39<00:00, 51.10it/s]\n",
      "  0%|          | 2/1563 [00:00<02:08, 12.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.540800014019\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1563/1563 [01:35<00:00, 16.31it/s]\n",
      "100%|██████████| 1250/1250 [01:56<00:00, 10.68it/s]\n",
      "100%|██████████| 2000/2000 [00:39<00:00, 50.99it/s]\n",
      "  0%|          | 2/1563 [00:00<01:56, 13.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.543050014093\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1563/1563 [01:35<00:00, 16.37it/s]\n",
      "100%|██████████| 1250/1250 [01:56<00:00, 10.69it/s]\n",
      "100%|██████████| 2000/2000 [00:38<00:00, 51.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.54491819594\n",
      "CPU times: user 34min 58s, sys: 11min 21s, total: 46min 20s\n",
      "Wall time: 41min 59s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "x = q_train[:100000]\n",
    "batch_size = 64\n",
    "\n",
    "for ep in range(1):\n",
    "\n",
    "    for i in tqdm(range(math.ceil(uns_q_train.shape[0]/batch_size))):\n",
    "        batch_q = uns_q_train[i*batch_size:(i+1)*batch_size].todense()\n",
    "        batch_d = uns_d_train[i*batch_size:(i+1)*batch_size].todense()\n",
    "\n",
    "        vae.model.train_on_batch(batch_d, [])\n",
    "    \n",
    "    train_mlp()\n",
    "    evaluate()\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras import backend as K\n",
    "from keras.models import Sequential,  Model\n",
    "from keras.layers.core import Dense, Lambda, Activation\n",
    "from keras.layers import Embedding, Input, Dense, merge, Reshape, Merge, Flatten, Dropout, GlobalAveragePooling1D\n",
    "from keras.constraints import maxnorm\n",
    "from keras.optimizers import Adagrad, Adam, SGD, RMSprop\n",
    "from time import time\n",
    "import sys\n",
    "\n",
    "def mlp(latent_dim):\n",
    "\n",
    "    que_input = Input(shape=(latent_dim,))\n",
    "    doc_input = Input(shape=(latent_dim,))\n",
    "\n",
    "    concat = merge([que_input, doc_input], mode=\"concat\")\n",
    "\n",
    "    d1 = Dense(512, activation='relu')\n",
    "    d2 = Dense(256, activation='relu')\n",
    "    d3 = Dense(128, activation='relu')\n",
    "    d4 = Dense(64, activation='relu')\n",
    "    d5 = Dense(32, activation='relu')\n",
    "    d6 = Dense(1, activation='sigmoid')\n",
    "\n",
    "    out = d6(d5(d4(d3(d2(d1(concat))))))\n",
    "\n",
    "    model = Model(input=[que_input, doc_input], output=out)\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_num = 20000\n",
    "sup_q_train = q_train[:sample_num-test_num]\n",
    "sup_d_train = d_train[:sample_num-test_num]\n",
    "sup_y_train = y_train[:sample_num-test_num]\n",
    "\n",
    "sup_q_test = q_train[sample_num-test_num:sample_num]\n",
    "sup_d_test = d_train[sample_num-test_num:sample_num]\n",
    "sup_y_test = y_train[sample_num-test_num:sample_num]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 74.9 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "def train_mlp(model):\n",
    "\n",
    "    batch_size = 64\n",
    "\n",
    "    for i in tqdm(range(math.ceil(sup_q_train.shape[0]/batch_size))):\n",
    "        batch_q = sup_q_train[i*batch_size:(i+1)*batch_size].todense()\n",
    "        batch_d = sup_d_train[i*batch_size:(i+1)*batch_size].todense()\n",
    "        batch_y = sup_y_train[i*batch_size:(i+1)*batch_size]\n",
    "\n",
    "    #   encode inputs\n",
    "\n",
    "        enc_q = vae.encoder.predict(batch_q)\n",
    "        enc_d = vae.encoder.predict(batch_d)\n",
    "\n",
    "        loss = model.train_on_batch([enc_q, enc_d], batch_y)\n",
    "#     print(\"\\r Loss:{:.3f}\".format(loss), end='')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mean_avg = []\n",
    "def evaluate_vae(model):\n",
    "    batch_size = 10\n",
    "    for i in tqdm(range(math.ceil(sup_q_test.shape[0]/batch_size))):\n",
    "        batch_q = sup_q_test[i*batch_size:(i+1)*batch_size].todense()\n",
    "        batch_d = sup_d_test[i*batch_size:(i+1)*batch_size].todense()\n",
    "        batch_y = sup_y_test[i*batch_size:(i+1)*batch_size]\n",
    "\n",
    "        enc_q = vae.encoder.predict(batch_q)\n",
    "        enc_d = vae.encoder.predict(batch_d)\n",
    "\n",
    "        avg = model.evaluate([enc_q, enc_d], batch_y, verbose=0)\n",
    "        mean_avg.append(avg[1])\n",
    "    print(np.average(mean_avg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train_mlp(model):\n",
    "\n",
    "    batch_size = 1000\n",
    "\n",
    "    for i in tqdm(range(math.ceil(sup_q_train.shape[0]/batch_size))):\n",
    "        batch_q = sup_q_train[i*batch_size:(i+1)*batch_size].todense()\n",
    "        batch_d = sup_d_train[i*batch_size:(i+1)*batch_size].todense()\n",
    "        batch_y = sup_y_train[i*batch_size:(i+1)*batch_size]\n",
    "\n",
    "        loss = model.train_on_batch([batch_q, batch_d], batch_y)\n",
    "\n",
    "def evaluate_mlp(model):\n",
    "    batch_size = 10\n",
    "    for i in tqdm(range(math.ceil(sup_q_test.shape[0]/batch_size))):\n",
    "        batch_q = sup_q_test[i*batch_size:(i+1)*batch_size].todense()\n",
    "        batch_d = sup_d_test[i*batch_size:(i+1)*batch_size].todense()\n",
    "        batch_y = sup_y_test[i*batch_size:(i+1)*batch_size]\n",
    "\n",
    "        avg = model.evaluate([batch_q, batch_d], batch_y, verbose=0)\n",
    "        mean_avg.append(avg[1])\n",
    "    print(np.average(mean_avg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda3/lib/python3.6/site-packages/ipykernel/__main__.py:16: UserWarning: The `merge` function is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "/home/t-jamano/.local/lib/python3.6/site-packages/keras/legacy/layers.py:464: UserWarning: The `Merge` layer is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "  name=name)\n",
      "/usr/local/anaconda3/lib/python3.6/site-packages/ipykernel/__main__.py:27: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=[<tf.Tenso..., outputs=Tensor(\"de...)`\n",
      "\n",
      "  0%|          | 0/80 [00:00<?, ?it/s]\u001b[A\n",
      "100%|██████████| 80/80 [01:15<00:00,  1.05it/s]\n",
      "100%|██████████| 2000/2000 [00:41<00:00, 48.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.655575016832\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "tf_idf = mlp(50000)\n",
    "train_mlp(tf_idf)\n",
    "evaluate_mlp(tf_idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.q.tolist() + df.d.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9999994"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df.q.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3090780 unique tokens\n",
      "Number of Vocab: 1000001\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "tokenizer = Tokenizer(MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(df.q.tolist() + df.d.tolist())\n",
    "word_index = tokenizer.word_index #the dict values start from 1 so this is fine with zeropadding\n",
    "index2word = {v: k for k, v in word_index.items()}\n",
    "print('Found %s unique tokens' % len(word_index))\n",
    "NB_WORDS = (min(tokenizer.num_words, len(word_index)) + 1 ) #+1 for zero padding\n",
    "print('Number of Vocab: %d' % NB_WORDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# saving\n",
    "with open('/home/t-jamano/data/10M_query_title_tokenizer.pickle', 'wb') as handle:\n",
    "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "# loading\n",
    "with open('/home/t-jamano/data/10M_query_title_tokenizer.pickle', 'rb') as handle:\n",
    "    tokenizer = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sequences = tokenizer.texts_to_sequences(df.q.tolist())\n",
    "data_train = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "d_sequences = tokenizer.texts_to_sequences(df.d.tolist())\n",
    "d_data_train = pad_sequences(d_sequences, maxlen=MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.save('/home/t-jamano/data/10M_query_token', data_train)\n",
    "np.save('/home/t-jamano/data/10M_title_token', d_data_train)\n",
    "np.save('/home/t-jamano/data/10M_labels', df.label.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  0,  0,  0, 14, 42,  2], dtype=int32)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# np.load('/home/t-jamano/data/10M_query_token.npy')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "GLOVE_EMBEDDING = '/home/t-jamano/data/glove/glove.6B.50d.txt'\n",
    "embeddings_index = {}\n",
    "f = open(GLOVE_EMBEDDING, encoding='utf8')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))\n",
    "\n",
    "EMBEDDING_DIM = 50\n",
    "\n",
    "glove_embedding_matrix = np.zeros((NB_WORDS, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    if i < NB_WORDS:\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            # words not found in embedding index will be the word embedding of 'unk'.\n",
    "            glove_embedding_matrix[i] = embedding_vector\n",
    "        else:\n",
    "            glove_embedding_matrix[i] = embeddings_index.get('unk')\n",
    "print('Null word embeddings: %d' % np.sum(np.sum(glove_embedding_matrix, axis=1) == 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.save('/home/t-jamano/data/10M_glove_embedding_matrix', glove_embedding_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "glove_embedding_matrix = np.load('/home/t-jamano/data/10M_glove_embedding_matrix.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "NB_WORDS = 1000000 + 1\n",
    "EMBEDDING_DIM = 50\n",
    "MAX_SEQUENCE_LENGTH = 7\n",
    "embed = Embedding(NB_WORDS, EMBEDDING_DIM, weights=[glove_embedding_matrix],\n",
    "                            input_length=MAX_SEQUENCE_LENGTH, trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def w2v(input_dim):\n",
    "    \n",
    "    que_input = Input(shape=(input_dim,))\n",
    "    doc_input = Input(shape=(input_dim,))\n",
    "    \n",
    "    embed_q = Flatten()(embed(que_input))\n",
    "    embed_d = Flatten()(embed(doc_input))\n",
    "    \n",
    "    \n",
    "\n",
    "    concat = merge([que_input, doc_input], mode=\"concat\")\n",
    "\n",
    "    d1 = Dense(512, activation='relu')\n",
    "    d2 = Dense(256, activation='relu')\n",
    "    d3 = Dense(128, activation='relu')\n",
    "    d4 = Dense(64, activation='relu')\n",
    "    d5 = Dense(32, activation='relu')\n",
    "    d6 = Dense(1, activation='sigmoid')\n",
    "\n",
    "    out = d6(d5(d4(d3(d2(d1(concat))))))\n",
    "\n",
    "    model = Model(input=[que_input, doc_input], output=out)\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda3/lib/python3.6/site-packages/ipykernel/__main__.py:11: UserWarning: The `merge` function is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "/home/t-jamano/.local/lib/python3.6/site-packages/keras/legacy/layers.py:464: UserWarning: The `Merge` layer is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "  name=name)\n",
      "/usr/local/anaconda3/lib/python3.6/site-packages/ipykernel/__main__.py:22: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=[<tf.Tenso..., outputs=Tensor(\"de...)`\n"
     ]
    }
   ],
   "source": [
    "w2v_model = w2v(MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "query = np.load('/home/t-jamano/data/10M_query_token.npy')\n",
    "title = np.load('/home/t-jamano/data/10M_title_token.npy')\n",
    "label = np.load('/home/t-jamano/data/10M_labels.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6699995 samples, validate on 3299999 samples\n",
      "Epoch 1/2\n",
      " - 166s - loss: 6.0979 - acc: 0.5319 - val_loss: 0.6804 - val_acc: 0.5304\n",
      "Epoch 2/2\n",
      " - 165s - loss: 0.6922 - acc: 0.5307 - val_loss: 0.6913 - val_acc: 0.5304\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fc9e0aafb38>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.fit([query, title], label, verbose=2, batch_size=256, epochs=2, validation_split=0.33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sup_sample_num = 1000000\n",
    "sup_query = query[:sup_sample_num]\n",
    "sup_title = title[:sup_sample_num]\n",
    "sup_label = label[:sup_sample_num]\n",
    "\n",
    "uns_doc = np.concatenate((query[sup_sample_num:], title[sup_sample_num:]))\n",
    "uns_label = label[sup_sample_num:]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NB_WORDS = 1000000 + 1\n",
    "EMBEDDING_DIM = 50\n",
    "MAX_SEQUENCE_LENGTH = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 7) (?, 7, 1000001)\n"
     ]
    }
   ],
   "source": [
    "batch_size = 256\n",
    "max_len = MAX_SEQUENCE_LENGTH\n",
    "emb_dim = EMBEDDING_DIM\n",
    "latent_dim = 200\n",
    "intermediate_dim = 64\n",
    "epsilon_std = 1.0\n",
    "num_sampled=500\n",
    "act = ELU()\n",
    "\n",
    "#y = Input(batch_shape=(None, max_len, NB_WORDS))\n",
    "x = Input(batch_shape=(None, max_len))\n",
    "x_embed = embed(x)\n",
    "h = LSTM(intermediate_dim, return_sequences=False, recurrent_dropout=0.2)(x_embed)\n",
    "# h = Dropout(0.2)(h)\n",
    "# h = Dense(intermediate_dim, activation='linear')(h)\n",
    "# h = act(h)\n",
    "# h = Dropout(0.2)(h)\n",
    "z_mean = Dense(latent_dim)(h)\n",
    "z_log_var = Dense(latent_dim)(h)\n",
    "\n",
    "def sampling(args):\n",
    "    z_mean, z_log_var = args\n",
    "    epsilon = K.random_normal(shape=(batch_size, latent_dim), mean=0.,\n",
    "                              stddev=epsilon_std)\n",
    "    return z_mean + K.exp(z_log_var / 2) * epsilon\n",
    "\n",
    "# note that \"output_shape\" isn't necessary with the TensorFlow backend\n",
    "z = Lambda(sampling, output_shape=(latent_dim,))([z_mean, z_log_var])\n",
    "\n",
    "# we instantiate these layers separately so as to reuse them later\n",
    "repeated_context = RepeatVector(max_len)\n",
    "decoder_h = LSTM(intermediate_dim, return_sequences=True, recurrent_dropout=0.2)\n",
    "decoder_mean = TimeDistributed(Dense(NB_WORDS, activation='linear'))#softmax is applied in the seq2seqloss by tf\n",
    "h_decoded = decoder_h(repeated_context(z))\n",
    "x_decoded_mean = decoder_mean(h_decoded)\n",
    "\n",
    "\n",
    "# placeholder loss\n",
    "def zero_loss(y_true, y_pred):\n",
    "    return K.zeros_like(y_pred)\n",
    "\n",
    "# Custom VAE loss layer\n",
    "class CustomVariationalLayer(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        self.is_placeholder = True\n",
    "        super(CustomVariationalLayer, self).__init__(**kwargs)\n",
    "        self.target_weights = tf.constant(np.ones((batch_size, max_len)), tf.float32)\n",
    "\n",
    "    def vae_loss(self, x, x_decoded_mean):\n",
    "        #xent_loss = K.sum(metrics.categorical_crossentropy(x, x_decoded_mean), axis=-1)\n",
    "        labels = tf.cast(x, tf.int32)\n",
    "        xent_loss = K.sum(tf.contrib.seq2seq.sequence_loss(x_decoded_mean, labels, \n",
    "                                                     weights=self.target_weights,\n",
    "                                                     average_across_timesteps=False,\n",
    "                                                     average_across_batch=False), axis=-1)\n",
    "                                                     #softmax_loss_function=softmax_loss_f), axis=-1)#, uncomment for sampled softmax\n",
    "        kl_loss = - 0.5 * K.sum(1 + z_log_var - K.square(z_mean) - K.exp(z_log_var), axis=-1)\n",
    "        return K.mean(xent_loss + kl_loss)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = inputs[0]\n",
    "        x_decoded_mean = inputs[1]\n",
    "        print(x.shape, x_decoded_mean.shape)\n",
    "        loss = self.vae_loss(x, x_decoded_mean)\n",
    "        self.add_loss(loss, inputs=inputs)\n",
    "        # we don't use this output, but it has to have the correct shape:\n",
    "        return K.ones_like(x)\n",
    "\n",
    "loss_layer = CustomVariationalLayer()([x, x_decoded_mean])\n",
    "vae = Model(x, [loss_layer])\n",
    "# opt = Adam(lr=0.01) #SGD(lr=1e-2, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "vae.compile(optimizer='adam', loss=[zero_loss])\n",
    "# vae.summary()\n",
    "\n",
    "# build a model to project sentences on the latent space\n",
    "encoder = Model(x, z_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[     0,      0,      0,      0,   4221,  30633, 350544],\n",
       "       [     0,      0,      0,   1962,    710,     40,   5543],\n",
       "       [     0,      0,      0,      0,      0,      0, 891415],\n",
       "       [     0,      0,      0,      0,      0,      0,   1669],\n",
       "       [     0,      0,      0,  16191,   2921,     11,     10],\n",
       "       [     0,      0,      0,   2063,    421, 177638,     40],\n",
       "       [     0,      0,      0,      0,      0,    720, 891416],\n",
       "       [     0,      0,      0,      0,     81,     36,      2],\n",
       "       [     0,      0,      0,      0,      0,   6288,   9574],\n",
       "       [     0,      0,      0,      0,    584,  91076,    383]], dtype=int32)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uns_doc[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 14399990 samples, validate on 3599998 samples\n",
      "Epoch 1/2\n",
      " - 326s - loss: -2.1132e+06 - val_loss: -2.2566e+06\n",
      "Epoch 2/2\n",
      " - 327s - loss: -2.1133e+06 - val_loss: -2.2566e+06\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fc4731e9c50>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vae.model.fit(uns_doc, verbose=2, batch_size=256, epochs=2, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda3/lib/python3.6/site-packages/ipykernel/__main__.py:16: UserWarning: The `merge` function is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "/home/t-jamano/.local/lib/python3.6/site-packages/keras/legacy/layers.py:464: UserWarning: The `Merge` layer is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "  name=name)\n",
      "/usr/local/anaconda3/lib/python3.6/site-packages/ipykernel/__main__.py:27: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=[<tf.Tenso..., outputs=Tensor(\"de...)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6699995 samples, validate on 3299999 samples\n",
      "Epoch 1/2\n",
      " - 195s - loss: 0.6913 - acc: 0.5306 - val_loss: 0.6913 - val_acc: 0.5304\n",
      "Epoch 2/2\n",
      " - 194s - loss: 0.6913 - acc: 0.5307 - val_loss: 0.6913 - val_acc: 0.5304\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fc472c37f60>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vae_mlp_model = mlp(latent_dim)\n",
    "enc_q = vae.encoder.predict(query)\n",
    "enc_d = vae.encoder.predict(title)\n",
    "vae_mlp_model.fit([enc_q, enc_d], label, verbose=2, batch_size=256, epochs=2, validation_split=0.33)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
